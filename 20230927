These concepts are related to parallel computing. It would be helpful to learn a little about parallel computing, e.g., MPI.

You can think of world as a group containing all the processes for your distributed training. Usually, each GPU corresponds to one process. Processes in the world can communicate with each other, which is why you can train your model distributedly and still get the correct gradient update. So world size is the number of processes for your training, which is usually the number of GPUs you are using for distributed training.

Rank is the unique ID given to a process, so that other processes know how to identify a particular process. Local rank is the a unique local ID for processes running in a single node, this is where my view differs with @zihaozhihao.0

Let's take a concrete example. Suppose we run our training in 2 servers or nodes and each with 4 GPUs. The world size is 4*2=8. The ranks for the processes will be [0, 1, 2, 3, 4, 5, 6, 7]. In each node, the local rank will be [0, 1, 2, 3].



group convoluntion 
https://blog.csdn.net/qq_35608277/article/details/117447680?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-117447680-blog-80030468.235^v38^pc_relevant_anti_vip&spm=1001.2101.3001.4242.1&utm_relevant_index=3

https://blog.csdn.net/weixin_43609185/article/details/122071773?spm=1001.2101.3001.6650.15&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-15-122071773-blog-117447680.235%5Ev38%5Epc_relevant_anti_vip&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-15-122071773-blog-117447680.235%5Ev38%5Epc_relevant_anti_vip&utm_relevant_index=24


batch norm vs layer norm : https://blog.csdn.net/scar2016/article/details/123173948


# class 
class has 2 types of attribute (property) and methods / functions 
```
dir(Class) # get the list name of property and methods 
Class.__dict__: get the property dict 

[x for x in dir(a_class) if x not in a_class.__dict__] #get the functions 

```


# apply to ur code 
- activation function
    - on block 
    - the 1st layer 
- distributed learning 
- moving average for weigths 
combine batchnorm to conv: 
- understand batchnorm: https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739